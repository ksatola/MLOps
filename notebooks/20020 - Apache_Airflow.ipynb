{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "From: https://github.com/ksatola\n",
    "Version: 0.0.1\n",
    "\n",
    "TODOs\n",
    "1. \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Airflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "- [Jupyter Lab and Python Environment Setup](#toc00)\n",
    "- [Airflow code examples](#toc01)\n",
    "- [Python example](#toc02)\n",
    "\n",
    "Resources:\n",
    "- https://airflow.apache.org/docs/apache-airflow/stable/installation/index.html\n",
    "- https://airflow.apache.org/docs/apache-airflow/stable/usage-cli.html\n",
    "- https://airflow.apache.org/docs/apache-airflow/stable/start/local.html\n",
    "- https://towardsdatascience.com/master-apache-airflow-how-to-install-and-setup-the-environment-in-10-minutes-61dad52d5239"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='toc00'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Lab and Python Environment Setup\n",
    "```\n",
    "# In WSL Terminal\n",
    "IMAGE_NAME='ksatola/ubuntu-python-dev-base'\n",
    "CONTAINER_NAME='ubuntu-python-dev-base-apache-airflow'\n",
    "\n",
    "docker run -d -t -P \\\n",
    "    --name $CONTAINER_NAME \\\n",
    "    --mount src='/home/ksatola/git',target='/root/git',type=bind \\\n",
    "    $IMAGE_NAME\n",
    "\n",
    "\n",
    "# Connect to the container with VSC with Remote Explorer\n",
    "\n",
    "# Dependencies\n",
    "\n",
    "# -----\n",
    "# Airflow locally\n",
    "# https://airflow.apache.org/docs/apache-airflow/stable/start/local.html\n",
    "\n",
    "# airflow needs a home, ~/airflow is the default,\n",
    "# but you can lay foundation somewhere else if you prefer\n",
    "# (optional)\n",
    "export AIRFLOW_HOME=~/airflow\n",
    "\n",
    "AIRFLOW_VERSION=2.1.4\n",
    "\n",
    "PYTHON_VERSION=\"$(python --version | cut -d \" \" -f 2 | cut -d \".\" -f 1-2)\"\n",
    "echo $PYTHON_VERSION\n",
    "# For example: 3.9\n",
    "\n",
    "CONSTRAINT_URL=\"https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-${PYTHON_VERSION}.txt\"\n",
    "# For example: https://raw.githubusercontent.com/apache/airflow/constraints-2.1.4/constraints-3.6.txt\n",
    "\n",
    "pip install \"apache-airflow==${AIRFLOW_VERSION}\" --constraint \"${CONSTRAINT_URL}\"\n",
    "\n",
    "# initialize the database\n",
    "airflow db init\n",
    "\n",
    "airflow users create \\\n",
    "    --username admin \\\n",
    "    --firstname FIRST_NAME \\\n",
    "    --lastname LAST_NAME \\\n",
    "    --role Admin \\\n",
    "    --email admin@example.org\n",
    "password: admin\n",
    "\n",
    "# start the web server, default port is 8080\n",
    "airflow webserver --port 8085\n",
    "\n",
    "# start the scheduler\n",
    "# open a new terminal or else run webserver with ``-D`` option to run it as a daemon\n",
    "airflow scheduler\n",
    "\n",
    "# visit localhost:8085 in the browser and use the admin account you just\n",
    "# created to login. Enable the example_bash_operator dag in the home page\n",
    "\n",
    "#Upon running these commands, Airflow will create the $AIRFLOW_HOME folder and create the “airflow.cfg” file with defaults that will get you going fast. \n",
    "# You can inspect the file either in $AIRFLOW_HOME/airflow.cfg, or through the UI in the Admin->Configuration menu. The PID file for the webserver will be stored in $AIRFLOW_HOME/airflow-webserver.pid or in /run/airflow/webserver.pid if started by systemd.\n",
    "\n",
    "airflow dags list\n",
    "\n",
    "# run your first task instance\n",
    "airflow tasks run example_bash_operator runme_0 2015-01-01\n",
    "\n",
    "# run a backfill over 2 days\n",
    "airflow dags backfill example_bash_operator \\\n",
    "    --start-date 2015-01-01 \\\n",
    "    --end-date 2015-01-02\n",
    "# -----\n",
    "\n",
    "\n",
    "pip install jupyterlab\n",
    "\n",
    "# Run in the container\n",
    "jupyter lab --no-browser --allow-root\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='toc01'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the DAG object\n",
    "from datetime import datetime\n",
    "from airflow.models import DAG\n",
    "\n",
    "# Define the default_args dictionary\n",
    "default_args = {\n",
    "  'owner': 'dsmith',\n",
    "  'start_date': datetime(2020, 1, 14),\n",
    "  'retries': 2\n",
    "}\n",
    "\n",
    "# Instantiate the DAG object\n",
    "etl_dag = DAG('example_etl', default_args=default_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bash Workflow Operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the BashOperator\n",
    "from airflow.operators.bash import BashOperator\n",
    "\n",
    "# Define the BashOperator \n",
    "cleanup = BashOperator(\n",
    "    task_id='cleanup_task',\n",
    "    # Define the bash_command\n",
    "    bash_command='cleanup.sh',\n",
    "    # Add the task to the dag\n",
    "    dag=etl_dag\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "consolidate = BashOperator(\n",
    "    task_id='consolidate_task',\n",
    "    # Define the bash_command\n",
    "    bash_command='consolidate.sh',\n",
    "    # Add the task to the dag\n",
    "    dag=etl_dag\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pull_sales = BashOperator(\n",
    "    task_id='pullsales_task',\n",
    "    bash_command='wget https://salestracking/latestinfo?json',\n",
    "    dag=etl_dag\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "push_data = BashOperator(\n",
    "    task_id='pushdata_task',\n",
    "    bash_command='push.sh',\n",
    "    dag=etl_dag\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks Order (upstream/prior, downstream/following)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set pull_sales to run prior to cleanup\n",
    "pull_sales >> cleanup\n",
    "\n",
    "# Configure consolidate to run after cleanup\n",
    "cleanup >> consolidate\n",
    "\n",
    "# Set push_data to run last\n",
    "consolidate >> push_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python Workflow Operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.operators.python import PythonOperator\n",
    "\n",
    "def pull_file(URL, savepath):\n",
    "    r = requests.get(URL)\n",
    "    with open(savepath, 'wb') as f:\n",
    "        f.write(r.content)   \n",
    "    # Use the print method for logging\n",
    "    print(f\"File pulled from {URL} and saved to {savepath}\")\n",
    "\n",
    "# Create the task\n",
    "pull_file_task = PythonOperator(\n",
    "    task_id='pull_file',\n",
    "    # Add the callable\n",
    "    python_callable=pull_file,\n",
    "    # Define the arguments\n",
    "    op_kwargs={'URL':'http://dataserver/sales.json', 'savepath':'latestsales.json'},\n",
    "    dag=etl_dag\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the Operator\n",
    "from airflow.operators.email import EmailOperator\n",
    "\n",
    "# Define the task\n",
    "email_manager_task = EmailOperator(\n",
    "    task_id='email_manager',\n",
    "    to='manager@datacamp.com',\n",
    "    subject='Latest sales JSON',\n",
    "    html_content='Attached is the latest sales JSON file as requested.',\n",
    "    files='parsedfile.json',\n",
    "    dag=etl_dag\n",
    ")\n",
    "\n",
    "# Set the order of tasks\n",
    "pull_file_task >> email_manager_task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DAGs Scheduling\n",
    "\n",
    "<img src=\"images/cron-job-format-1.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "See: [crontab.guru](https://crontab.guru/#0_*_*_*_*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta\n",
    "\n",
    "# Update the scheduling arguments as defined\n",
    "default_args = {\n",
    "  'owner': 'Engineering',\n",
    "  'start_date': datetime(2019, 11, 1),\n",
    "  'email': ['airflowresults@datacamp.com'],\n",
    "  'email_on_failure': False,\n",
    "  'email_on_retry': False,\n",
    "  'retries': 3,\n",
    "  'retry_delay': timedelta(minutes=20)\n",
    "}\n",
    "\n",
    "dag = DAG('update_dataflows', default_args=default_args, schedule_interval='30 12 * * 3')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sensor Operator (Conditions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.models import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.contrib.sensors.file_sensor import FileSensor\n",
    "from datetime import datetime\n",
    "\n",
    "report_dag = DAG(\n",
    "    dag_id = 'execute_report',\n",
    "    schedule_interval = \"0 0 * * *\"\n",
    ")\n",
    "\n",
    "precheck = FileSensor(\n",
    "    task_id='check_for_datafile',\n",
    "    filepath='salesdata_ready.csv',\n",
    "    start_date=datetime(2020,2,20),\n",
    "    mode='poke',\n",
    "    dag=report_dag\n",
    ")\n",
    "\n",
    "generate_report_task = BashOperator(\n",
    "    task_id='generate_report',\n",
    "    bash_command='generate_report.sh',\n",
    "    start_date=datetime(2020,2,20),\n",
    "    dag=report_dag\n",
    ")\n",
    "\n",
    "precheck >> generate_report_task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='toc02'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python example\n",
    "Run python function in a sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_task(desc):\n",
    "    print(desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_args = {\n",
    "  'owner': 'admin',\n",
    "  'start_date': datetime(2021, 9, 26),\n",
    "  'retries': 2\n",
    "}\n",
    "\n",
    "test_dag = DAG('test_dag', default_args=default_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from airflow.operators.python import PythonOperator\n",
    "\n",
    "task1 = PythonOperator(\n",
    "    task_id='t1',\n",
    "    python_callable=execute_task,\n",
    "    op_kwargs={'desc':'Task 1 executed first'},\n",
    "    dag=test_dag\n",
    ")\n",
    "\n",
    "task2 = PythonOperator(\n",
    "    task_id='t2',\n",
    "    python_callable=execute_task,\n",
    "    op_kwargs={'desc':'Task 2 executed as 3rd'},\n",
    "    dag=test_dag\n",
    ")\n",
    "\n",
    "task3 = PythonOperator(\n",
    "    task_id='t3',\n",
    "    python_callable=execute_task,\n",
    "    op_kwargs={'desc':'Task 3 executed as 2nd'},\n",
    "    dag=test_dag\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task1 >> task3 >> task2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir ~/airflow/dags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_test_dag_file = f'~/airflow/dags/test_dag.py'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile {_test_dag_file}\n",
    "\n",
    "# https://github.com/hgrif/airflow-tutorial\n",
    "# Create a DAG file in AIRFLOW_HOME/dags\n",
    "# Create test_dag.py\n",
    "\n",
    "import datetime as dt\n",
    "\n",
    "from airflow import DAG\n",
    "from airflow.operators.bash import BashOperator\n",
    "from airflow.operators.python import PythonOperator\n",
    "\n",
    "\n",
    "def print_world():\n",
    "    print('world')\n",
    "\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'me',\n",
    "    'start_date': dt.datetime(2021, 9, 26),\n",
    "    'retries': 1,\n",
    "    'retry_delay': dt.timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "\n",
    "with DAG('test_dag',\n",
    "         default_args=default_args,\n",
    "         schedule_interval='0 * * * *',\n",
    "         ) as dag:\n",
    "\n",
    "    print_hello = BashOperator(task_id='print_hello',\n",
    "                               bash_command='echo \"hello\"')\n",
    "    sleep = BashOperator(task_id='sleep',\n",
    "                         bash_command='sleep 5')\n",
    "    print_world = PythonOperator(task_id='print_world',\n",
    "                                 python_callable=print_world)\n",
    "\n",
    "\n",
    "print_hello >> sleep >> print_world\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Run in container's terminal\n",
    "\n",
    "cd ~/airflow/dags\n",
    "python test_dag.py\n",
    "\n",
    "# Test the DAG\n",
    "airflow tasks test test_dag print_world 2021-09-26\n",
    "\n",
    "# Run\n",
    "airflow scheduler\n",
    "\n",
    "# Check in the web UI\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "interactive_pipeline.ipynb",
   "provenance": []
  },
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
