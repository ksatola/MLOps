{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "From: https://github.com/ksatola\n",
    "Version: 0.0.1\n",
    "\n",
    "TODOs\n",
    "1. \n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "- [Jupyter Lab and Python Environment Setup](#toc00)\n",
    "- [XXXXAirflow code examples](#toc01)\n",
    "- [XXXXPython example](#toc02)\n",
    "\n",
    "Resources:\n",
    "- https://mlflow.org/\n",
    "- https://scikit-learn.org/stable/datasets/toy_dataset.html\n",
    "- https://devopscube.com/run-docker-in-docker/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='toc00'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jupyter Lab and Python Environment Setup\n",
    "```\n",
    "# -----\n",
    "# In WSL Terminal\n",
    "IMAGE_NAME='ksatola/ubuntu-python-dev-base' \\\n",
    "CONTAINER_NAME='ubuntu-python-dev-base-mlflow'\n",
    "\n",
    "# !!! In order to be able to connect to Docker on WSL, the second line (starting with -v) is needed\n",
    "# https://devopscube.com/run-docker-in-docker/\n",
    "docker run -d -t -P \\\n",
    "    -v /var/run/docker.sock:/var/run/docker.sock \\\n",
    "    --name $CONTAINER_NAME \\\n",
    "    --mount src='/home/ksatola/git',target='/root/git',type=bind \\\n",
    "    $IMAGE_NAME\n",
    "\n",
    "# Connect to the container with VSC with Remote Explorer\n",
    "\n",
    "# -----\n",
    "# In the container Terminal\n",
    "\n",
    "# -----\n",
    "# Install miniconda (can be only done manually) - MLflow needs it\n",
    "# https://docs.conda.io/en/latest/miniconda.html\n",
    "\n",
    "# Download the latest shell script\n",
    "wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
    "\n",
    "# Make the miniconda installation script executable\n",
    "chmod +x Miniconda3-latest-Linux-x86_64.sh\n",
    "\n",
    "# Run miniconda installation script\n",
    "./Miniconda3-latest-Linux-x86_64.sh\n",
    "\n",
    "# Reload your shell\n",
    "exec \"$SHELL\" # Or just restart your terminal\n",
    "\n",
    "# Cleanup\n",
    "rm Miniconda3-latest-Linux-x86_64.sh\n",
    "\n",
    "# Create and activate an conda environment\n",
    "#conda create -n newenv\n",
    "\n",
    "\n",
    "# -----\n",
    "# MLFlow Installation\n",
    "pip install mlflow \\\n",
    "    pip install sklearn \\\n",
    "    pip install matplotlib \\\n",
    "    pip install pandas_datareader\n",
    "\n",
    "# -----\n",
    "# Install Docker for MLflow experiments\n",
    "# https://askubuntu.com/questions/1030179/package-docker-ce-has-no-installation-candidate-in-18-04\n",
    "\n",
    "apt update\n",
    "apt upgrade\n",
    "\n",
    "apt-get install -y \\\n",
    "    apt-transport-https \\\n",
    "    ca-certificates \\\n",
    "    curl \\\n",
    "    gnupg \\\n",
    "    lsb-release \\\n",
    "    software-properties-common\n",
    "\n",
    "echo \\\n",
    "  \"deb [arch=amd64 signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/debian \\\n",
    "  $(lsb_release -cs) stable\" | tee /etc/apt/sources.list.d/docker.list > /dev/null\n",
    "\n",
    "curl -fsSL https://download.docker.com/linux/ubuntu/gpg | gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg\n",
    "\n",
    "apt update\n",
    "    \n",
    "#apt install -y software-properties-common\n",
    "#curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add -\n",
    "\n",
    "add-apt-repository \"deb [arch=amd64] https://download.docker.com/linux/ubuntu `lsb_release -cs` test\"\n",
    "apt update\n",
    "apt install -y docker-ce\n",
    "docker -v\n",
    "\n",
    "# Check if there is connection to the WSL Docker\n",
    "docker images\n",
    "\n",
    "\n",
    "# -----\n",
    "# Docker compose\n",
    "# Docker Compose is yet another useful Docker tool. It allows users to launch, execute, communicate, \n",
    "# and close containers with a single coordinated command. Essentially, Docker Compose is used \n",
    "# for defining and running multi-container Docker applications.\n",
    "# https://phoenixnap.com/kb/install-docker-compose-on-ubuntu-20-04\n",
    "apt update\n",
    "apt upgrade\n",
    "\n",
    "apt install docker-compose\n",
    "ln -s /usr/local/bin/docker-compose /usr/bin/docker-compose\n",
    "#chmod +x /usr/local/bin/docker-compose\n",
    "\n",
    "/usr/local/bin/docker-compose --version\n",
    "\n",
    "#\"PATH=$PATH:/home/user/.local/bin\" docker-compose\n",
    "export PATH=\"/usr/local/bin/:$PATH\"\n",
    "\n",
    "#apt install curl\n",
    "\n",
    "#curl -L \"https://github.com/docker/compose/releases/download/1.29.2/docker-compose-$(uname -s)-$(uname -m)\" -o /usr/local/bin/docker-compose\n",
    "#chmod +x /usr/local/bin/docker-compose\n",
    "\n",
    "docker–compose --version\n",
    "\n",
    "# -----\n",
    "\n",
    "pip install jupyterlab\n",
    "\n",
    "# Run in the container\n",
    "jupyter lab --no-browser --allow-root\n",
    "\n",
    "# -----\n",
    "https://git-scm.com/book/en/v2/Getting-Started-Installing-Git\n",
    "apt update\n",
    "apt install git-all\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='toc01'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLflow and machine learning engineering practices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementing a product based on machine learning can be a laborious task. There is a general need to `reduce the friction between different steps of the machine learning\n",
    "development life cycle and between the teams of data scientists and engineers` that are involved in the process.\n",
    "\n",
    "Machine learning practitioners such as data scientists and machine learning engineers operate with different systems, standards, and tools. While data scientists spend most of their time developing models in tools such as Jupyter Notebook, when running in production, the model is deployed in the context of a software application with an environment that's more demanding in terms of scale and reliability.\n",
    "\n",
    "**MLflow** is an open source platform (created at Databricks) for the machine learning (ML) life cycle, with a focus on `reproducibility`, `training`, and `deployment`. It is based on an open interface design and is able to work with any language or platform, with clients in Python and Java, and is accessible through a REST API. Scalability is also an important benefit that an ML developer can leverage with MLflow.\n",
    "\n",
    "**MLflow** enables an everyday practitioner in one platform to manage the ML life cycle, from iteration on model development up to deployment in a reliable and scalable environment that is compatible with modern software system requirements.\n",
    "\n",
    "**MLflow** modules are software components that deliver the core features that aid in the different phases of the ML life cycle. MLflow features are delivered through modules, extensible components that organize related features in the platform. The following are the built-in modules in MLflow:\n",
    "- **MLflow Tracking:** Provides a mechanism and UI to handle metrics and artifacts generated by ML executions (training and inference).\n",
    "- **Mlflow Projects:** A package format to standardize ML projects. There are three different environments supported by MLflow projects: the Conda environment, Docker, and the local system.\n",
    "- **Mlflow Models:** A mechanism that deploys to different types of environments, both on-premises and in the cloud.\n",
    "- **Mlflow Model Registry:** A module that handles the management of models in MLflow and its life cycle, including state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='toc02'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple MLflow example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset = datasets.load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(dataset.data, dataset.target, test_size=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        21\n",
      "           1       0.82      1.00      0.90        18\n",
      "           2       1.00      0.81      0.89        21\n",
      "\n",
      "    accuracy                           0.93        60\n",
      "   macro avg       0.94      0.94      0.93        60\n",
      "weighted avg       0.95      0.93      0.93        60\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Train a model\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021/10/02 13:46:03 WARNING mlflow.utils.autologging_utils: You are using an unsupported version of sklearn. If you encounter errors during autologging, try upgrading / downgrading sklearn to a supported version, or try upgrading MLflow.\n",
      "/root/miniconda3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:814: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "2021/10/02 13:46:04 WARNING mlflow.utils.autologging_utils: MLflow autologging encountered a warning: \"/root/miniconda3/lib/python3.9/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function plot_confusion_matrix is deprecated; Function `plot_confusion_matrix` is deprecated in 1.0 and will be removed in 1.2. Use one of the class methods: ConfusionMatrixDisplay.from_predictions or ConfusionMatrixDisplay.from_estimator.\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        21\n",
      "           1       0.82      1.00      0.90        18\n",
      "           2       1.00      0.81      0.89        21\n",
      "\n",
      "    accuracy                           0.93        60\n",
      "   macro avg       0.94      0.94      0.93        60\n",
      "weighted avg       0.95      0.93      0.93        60\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# The same with MLflow\n",
    "import mlflow\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Log the experiment in the local directory (see mlruns folder)\n",
    "mlflow.sklearn.autolog()\n",
    "with mlflow.start_run():\n",
    "    clf = LogisticRegression()\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = clf.predict(X_test)\n",
    "\n",
    "    print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLflow version: 1.20.2\n",
      "sklearn version: 1.0\n",
      "matplotlib version: 3.4.3\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "print('MLflow version: {}'.format(mlflow.__version__))\n",
    "\n",
    "import sklearn\n",
    "print('sklearn version: {}'.format(sklearn.__version__))\n",
    "\n",
    "import matplotlib\n",
    "print('matplotlib version: {}'.format(matplotlib.__version__))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<a id='toc03'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLflow project to train a classifier\n",
    "The task in this illustrative project is to create a basic MLflow project and produce a working baseline ML model to predict, based on market signals over a certain number of days, whether the stock market will go up or down. \n",
    "\n",
    "We will use a Yahoo Finance dataset available for quoting the BTC-USD pair in https://finance.yahoo.com/quote/BTC-USD/ over a period of 3 months. We will train a model to predict whether the quote will be going up or not on a given day. A REST API will be made available for predictions through MLflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create mlflow_projects folder\n",
    "# Create project name folder (stockpred) inside the mlflow_projects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'~/git/MLOps/notebooks/mlflow_projects/'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "_path = f'~/git/MLOps/notebooks/mlflow_projects/'\n",
    "_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'~/git/MLOps/notebooks/mlflow_projects/stockpred/MLProject'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add MLProject file\n",
    "_mlproject_file = os.path.join(_path, \"stockpred/MLProject\")\n",
    "_mlproject_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /root/git/MLOps/notebooks/mlflow_projects/stockpred/MLProject\n"
     ]
    }
   ],
   "source": [
    "%%writefile {_mlproject_file}\n",
    "name: stockpred\n",
    "\n",
    "#conda_env: conda.yaml\n",
    "#docker_env:\n",
    "#  image:  ksatola/miniconda3\n",
    "\n",
    "entry_points:\n",
    "  main:\n",
    "    command: \"python train.py\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'~/git/MLOps/notebooks/mlflow_projects/stockpred/Dockerfile'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_dockerfile = os.path.join(_path, \"stockpred/Dockerfile\")\n",
    "_dockerfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Docker image file is based on the open source package **Miniconda**, a free minimal installer with a minimal set of packages for data science that allow us to control the details of the packages that we need in our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /root/git/MLOps/notebooks/mlflow_projects/stockpred/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile {_dockerfile}\n",
    "#FROM continuumio/miniconda:4.5.4\n",
    "FROM continuumio/miniconda3\n",
    "\n",
    "RUN pip install mlflow \\\n",
    "    && pip install numpy \\\n",
    "    && pip install scipy \\\n",
    "    && pip install pandas \\\n",
    "    && pip install scikit-learn \\\n",
    "    && pip install cloudpickle \\\n",
    "    && pip install pandas_datareader>=0.8.0\n",
    "\n",
    "#RUN pip install mlflow==1.2.0 \\\n",
    "#    && pip install numpy==1.14.3 \\\n",
    "#    && pip install scipy \\\n",
    "#    && pip install pandas==0.22.0 \\\n",
    "#    && pip install scikit-learn==0.20.4 \\\n",
    "#    && pip install cloudpickle \\\n",
    "#    && pip install pandas_datareader>=0.8.0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pandas_datareader.data as web\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import mlflow.sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data\n",
    "The format of the data acquired is the classic format for financial securities in exchange APIs. For every day of the period, we retrieve the following data: the highest value of the stock, the lowest, opening, and close values of the stock, as well as the volume. The final column represents the adjusted close value, the value after dividends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def acquire_training_data():\n",
    "    start = datetime.datetime(2019, 7, 1)\n",
    "    end = datetime.datetime(2019, 9, 30)\n",
    "    df = pdr.DataReader(\"BTC-USD\", 'yahoo', start, end)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make the data usable by scikit-learn\n",
    "Transform the raw data into a feature vector using the rolling window technique. The feature vector for each day becomes the deltas between the current and previous window days. In this case, we use the previous day's market movement (1 for a stock going up, 0 otherwise)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def digitize(n):\n",
    "    if n > 0:\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rolling_window(a, window):\n",
    "    \"\"\"\n",
    "        Takes np.array 'a' and size 'window' as parameters\n",
    "        Outputs an np.array with all the ordered sequences of values of 'a' of size 'window'\n",
    "        e.g. Input: ( np.array([1, 2, 3, 4, 5, 6]), 4 )\n",
    "             Output: \n",
    "                     array([[1, 2, 3, 4],\n",
    "                           [2, 3, 4, 5],\n",
    "                           [3, 4, 5, 6]])\n",
    "    \"\"\"\n",
    "    shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)\n",
    "    strides = a.strides + (a.strides[-1],)\n",
    "    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_training_data(data):\n",
    "    \"\"\"\n",
    "        Return a prepared numpy dataframe\n",
    "        input : Dataframe with expected schema\n",
    "    \"\"\"\n",
    "    data['Delta'] = data['Close'] - data['Open']\n",
    "    data['to_predict'] = data['Delta'].apply(lambda d: digitize(d))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and store your model in **MLflow**\n",
    "The `mlflow.sklearn.log_model(clf, \"model_random_forest\")` method takes care of persisting the model upon training. We are explicitly asking **MLflow** to log the model and the metrics that we find relevant. This flexibility in the items to log allows one program to log multiple models into **MLflow**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'~/git/MLOps/notebooks/mlflow_projects/stockpred/train.py'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add train.py file\n",
    "_tain_file = os.path.join(_path, \"stockpred/train.py\")\n",
    "_tain_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /root/git/MLOps/notebooks/mlflow_projects/stockpred/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile {_tain_file}\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pandas_datareader.data as web\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import mlflow.sklearn\n",
    "\n",
    "\n",
    "def acquire_training_data():\n",
    "    start = datetime.datetime(2019, 7, 1)\n",
    "    end = datetime.datetime(2019, 9, 30)\n",
    "    df = web.DataReader(\"BTC-USD\", 'yahoo', start, end)\n",
    "    return df\n",
    "\n",
    "def digitize(n):\n",
    "    if n > 0:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "\n",
    "def rolling_window(a, window):\n",
    "    \"\"\"\n",
    "        Takes np.array 'a' and size 'window' as parameters\n",
    "        Outputs an np.array with all the ordered sequences of values of 'a' of size 'window'\n",
    "        e.g. Input: ( np.array([1, 2, 3, 4, 5, 6]), 4 )\n",
    "             Output: \n",
    "                     array([[1, 2, 3, 4],\n",
    "                           [2, 3, 4, 5],\n",
    "                           [3, 4, 5, 6]])\n",
    "    \"\"\"\n",
    "    shape = a.shape[:-1] + (a.shape[-1] - window + 1, window)\n",
    "    strides = a.strides + (a.strides[-1],)\n",
    "    return np.lib.stride_tricks.as_strided(a, shape=shape, strides=strides)\n",
    "\n",
    "\n",
    "def prepare_training_data(data):\n",
    "    \"\"\"\n",
    "        Return a prepared numpy dataframe\n",
    "        input : Dataframe with expected schema\n",
    "    \"\"\"\n",
    "    data['Delta'] = data['Close'] - data['Open']\n",
    "    data['to_predict'] = data['Delta'].apply(lambda d: digitize(d))\n",
    "    return data\n",
    "\n",
    "WINDOW_SIZE = 14\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    #mlflow.set_tracking_uri(\"file:////root/git/MLOps/notebooks/mlflow_projects/stockpred\")\n",
    "    #with mlflow.run(uri='/root/git/MLOps/notebooks/mlflow_projects/stockpred/', experiment_name='aaa') as run:\n",
    "    #with mlflow.start_run(run_name='myrun') as run:\n",
    "    \n",
    "    mlflow.set_tracking_uri(\"http://localhost:5000\")\n",
    "    \n",
    "    with mlflow.start_run() as run:\n",
    "        training_data = acquire_training_data()\n",
    "        prepared_training_data_df = prepare_training_data(training_data)\n",
    "\n",
    "        btc_mat = prepared_training_data_df.to_numpy()\n",
    "\n",
    "        X = rolling_window(btc_mat[:, 7], WINDOW_SIZE)[:-1, :]\n",
    "        Y = prepared_training_data_df['to_predict'].to_numpy()[WINDOW_SIZE:]\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=4284, stratify=Y)\n",
    "\n",
    "        params = {\n",
    "            \"bootstrap\": \"True\",\n",
    "            \"criterion\": \"gini\",\n",
    "            \"min_samples_split\": 2,\n",
    "            \"min_weight_fraction_leaf\": 0.0,\n",
    "            \"n_estimators\": 50,\n",
    "            \"random_state\": 4284,\n",
    "            \"verbose\": 0,\n",
    "        }\n",
    "\n",
    "        clf = RandomForestClassifier(**params)\n",
    "        clf.fit(X_train, y_train)\n",
    "        predicted = clf.predict(X_test)\n",
    "\n",
    "        mlflow.log_params(params)\n",
    "        #mlflow.sklearn.log_model(clf, artifact_path=\"sklearn-model\")\n",
    "        #model_uri = \"runs:/{}/sklearn-model\".format(run.info.run_id)\n",
    "\n",
    "        #mv = mlflow.register_model(model_uri, \"RandomForestRegressionModel\")\n",
    "        #print(\"Name: {}\".format(mv.name))\n",
    "        #print(\"Version: {}\".format(mv.version))\n",
    "\n",
    "        print(classification_report(y_test, predicted))\n",
    "\n",
    "        mlflow.sklearn.log_model(clf, \"model_random_forest\")\n",
    "        mlflow.log_metric(\"precision_label_0\", precision_score(y_test, predicted, pos_label=0))\n",
    "        mlflow.log_metric(\"recall_label_0\", recall_score(y_test, predicted, pos_label=0))\n",
    "        mlflow.log_metric(\"f1score_label_0\", f1_score(y_test, predicted, pos_label=0))\n",
    "        mlflow.log_metric(\"precision_label_1\", precision_score(y_test, predicted, pos_label=1))\n",
    "        mlflow.log_metric(\"recall_label_1\", recall_score(y_test, predicted, pos_label=1))\n",
    "        mlflow.log_metric(\"f1score_label_1\", f1_score(y_test, predicted, pos_label=1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/root/git/MLOps/notebooks/mlflow_projects/stockpred'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir('/root/git/MLOps/notebooks/mlflow_projects/stockpred')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Start MLflow server in the Terminal\n",
    "cd /root/git/MLOps/notebooks/mlflow_projects/stockpred\n",
    "\n",
    "mlflow server \\\n",
    "    --backend-store-uri sqlite:///mlflow.db \\\n",
    "    --default-artifact-root ./mlruns \\\n",
    "    --host 0.0.0.0\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.61      1.00      0.76        11\n",
      "           1       1.00      0.22      0.36         9\n",
      "\n",
      "    accuracy                           0.65        20\n",
      "   macro avg       0.81      0.61      0.56        20\n",
      "weighted avg       0.79      0.65      0.58        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!python train.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# In the Terminal\n",
    "\n",
    "cd /root/git/MLOps/notebooks/mlflow_projects/stockpred\n",
    "mlflow server --backend-store-uri sqlite:///:memory --default-artifact-root ./mlruns\n",
    "\n",
    "# Serve model (builds conda environment for this)\n",
    "mlflow models serve -m ./mlruns/0/4f280fc7c70a432ba9e6902d2d634425/artifacts/model_random_forest/\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build your Docker image\n",
    "```\n",
    "cd ~/git/MLOps/notebooks/mlflow_projects/stockpred\n",
    "\n",
    "docker build \\\n",
    "    -f 'Dockerfile' \\\n",
    "    -t ksatola/miniconda3 .\n",
    "    \n",
    "#docker push ksatola/miniconda3\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run MLflow experiment inside Docker container\n",
    "```\n",
    "#docker login\n",
    "\n",
    "#mlflow ui --backend-store-uri file:///~/git/MLOps/notebooks/mlflow_projects/stockpred/mlruns\n",
    "#mlflow.set_tracking_uri(\"file:///~/git/MLOps/notebooks/mlflow_projects/stockpred/mlruns\")\n",
    "\n",
    "#mlflow run .\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMAGE_NAME='ksatola/miniconda:4.5.4' \\\n",
    "IMAGE_NAME='ksatola/miniconda3' \\\n",
    "CONTAINER_NAME='ksatola-mlflow-exp-test'\n",
    "\n",
    "# !!! In order to be able to connect to Docker on WSL, the second line (starting with -v) is needed\n",
    "# https://devopscube.com/run-docker-in-docker/\n",
    "docker run -d -t -P \\\n",
    "    -v /var/run/docker.sock:/var/run/docker.sock \\\n",
    "    --name $CONTAINER_NAME \\\n",
    "    --mount src='/root/git',target='/root/git',type=bind \\\n",
    "    $IMAGE_NAME\n",
    "    \n",
    "docker run -d -t -P \\\n",
    "    -v /var/run/docker.sock:/var/run/docker.sock \\\n",
    "    --name $CONTAINER_NAME \\\n",
    "    $IMAGE_NAME\n",
    "    \n",
    "docker run \\\n",
    "    --mount src='/root/git',target='/root/git',type=bind \\\n",
    "    -it ksatola/miniconda3 \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export MLFLOW_TRACKING_URI='~/git/MLOps/notebooks/mlflow_projects/stockpred/mlruns'\n",
    "#export MLFLOW_TRACKING_URI='~/git/MLOps/notebooks/mlflow_projects/stockpred'\n",
    "\n",
    "#export MLFLOW_CONDA_HOME\n",
    "\n",
    "#mlflow ui --backend-store-uri $MLFLOW_TRACKING_URI\n",
    "\n",
    "#mlflow run \"https://github.com/PacktPublishing/Machine-Learning-Engineering-with-MLflow/tree/master/Chapter01/stockpred\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mlflow\n",
    "\n",
    "class RandomPredictor(mlflow.pyfunc.PythonModel):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    # Implement the heuristic model in MLflow\n",
    "    def predict(self, context, model_input):\n",
    "        return model_input.apply(lambda column: random.randint(0,1))\n",
    "    \n",
    "    # Save the model in MLflow\n",
    "    model_path = \"random_model\"\n",
    "    baseline_model = RandomPredictor()\n",
    "    mlflow.pyfunc.save_model(path=model_path, python_model=random_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Run your mlflow job:\n",
    "mlflow run .\n",
    "\n",
    "Start the serving API:\n",
    "mlflow models serve -m ./mlruns/0/b9ee36e80a934cef9cac3a0513db515c/artifacts/random_model/\n",
    "\n",
    "Test the API of your model.\n",
    "You have access to a very simple Flask server that can run your model. You can test\n",
    "the execution by running a curl command in your server:\n",
    "curl http://127.0.0.1:5000/invocations -H 'Content-Type:application/json' -d '{\"data\":[[1,1,1,1,0,1,1,1,0,1,1,1,0,0]]}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Science Workbench\n",
    "In order to address common frictions for developing models in data science we need to provide data scientists and practitioners with a standardized environment in which they can develop and manage their work. A data science workbench should allow you to quick-start a project, and the availability of an environment with a set of starting tools and frameworks allows data scientists to rapidly jump-start a project.\n",
    "\n",
    "<img src=\"images/ds_workbench.png\" alt=\"\" style=\"width: 600px;\"/>\n",
    "\n",
    "We need the following core features in our data science workbench:\n",
    "- **Dependency Management:** Having dependency management built into your local environment helps in handling reproducibility issues and preventing library conflicts between different environments. This is generally achieved by using environment managers such as Docker or having environment management frameworks available in your programming language. MLflow provides this through the support of Docker- or Conda-based environments.\n",
    "- **Data Management:** Managing data in a local environment can be complex and daunting if you have to handle huge datasets. Having a standardized definition of how you handle data in your local projects allows others to freely collaborate on your projects and understand the structures available.\n",
    "- **Model Management:** Having the different models organized and properly stored provides an easy structure to be able to work through many ideas at the same time and persist the ones that have potential. MLflow helps support this through the model format abstraction and Model Registry component to manage models.\n",
    "- **Deployment:** Having a development environment aligned with the production environment where the model will be serviced requires deliberation in the local environment. The production environment needs to be ready to receive a model from a model developer, with the least possible friction. This smooth deployment workflow is only possible if the local environment is engineered correctly.\n",
    "- **Experimentation Management:** Tweaking parameters is the most common thing that a machine learning practitioner does. Being able to keep abreast of the different versions and specific parameters can quickly become cumbersome for the model developer.\n",
    "\n",
    "We will have the following components in the architecture of our development environment:\n",
    "- **Docker/Docker Compose:** Docker will be used to handle each of the main component dependencies of the architecture, and Docker Compose will be used as a coordinator between different containers of software pieces. The advantage of having each component of the workbench architecture in Docker is that neither element’s libraries will conflict with the other.\n",
    "- **JupyterLab:** The de facto environment to develop data science code and analytics in the context of machine learning.\n",
    "- **MLflow:** MLflow is at the cornerstone of the workbench, providing facilities for experiment tracking, model management, registry, and deployment interface.\n",
    "- **PostgreSQL database:** The PostgreSQL database is part of the architecture at this stage, as the storage layer for MLflow for backend metadata. Other relational databases could be used as the MLflow backend for metadata, but we will use PostgreSQL."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "# Copy the contents of the project\n",
    "https://github.com/PacktPublishing/Machine-Learning-Engineering-with-MLflow/tree/master/Chapter03/gradflow\n",
    "\n",
    "# In the WSL Ubuntu\n",
    "cd ~/git/gradflow\n",
    "\n",
    "# Start your local environment\n",
    "make\n",
    "\n",
    "# Inspect the created environments (should be 3 containers)\n",
    "docker ps\n",
    "\n",
    "# Open: http://localhost:8888/lab/workspaces/auto-L\n",
    "# Open: http://localhost:5000/#/\n",
    "\n",
    "# Tear dwon the environment\n",
    "make down\n",
    "\n",
    "```\n",
    "The usual ports used by your workbench are listed as follows: \n",
    "- Jupyter serves in port 8888, \n",
    "- MLflow serves in port 5000, \n",
    "- and PostgreSQL serves in port 5432."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ~/git/Machine-Learning-Engineering-with-MLflow-master/Chapter04/gradflow/\n",
    "make\n",
    "make down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cd ~/git/Machine-Learning-Engineering-with-MLflow-master/Chapter05/gradflow/\n",
    "make\n",
    "make down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "interactive_pipeline.ipynb",
   "provenance": []
  },
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
